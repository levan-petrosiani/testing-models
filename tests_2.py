import requests
import json
import time
from datetime import datetime

OLLAMA_URL_CHAT = "http://localhost:11434/api/chat"
OLLAMA_URL_GEN  = "http://localhost:11434/api/generate"
MODELS = ["qwen2.5:32b"]

timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
filename = f"results/{timestamp}-test.md"

test_cases = [
    "рЃЏрЃљрЃю рЃЏрЃћ рЃЏрЃЮрЃЏрЃфрЃљ рЃгрЃўрЃњрЃюрЃў.",
    "рЃЊрЃћрЃЊрЃљрЃЏ рЃерЃЋрЃўрЃџрЃА рЃЎрЃљрЃЉрЃљ рЃерЃћрЃБрЃЎрЃћрЃарЃљ.",
    "рЃЏрЃћ рЃерЃћрЃю рЃњрЃ«рЃћрЃЊрЃљрЃЋ рЃерЃћрЃю.",
    "рЃЕрЃЋрЃћрЃю рЃЏрЃљрЃЌ рЃЋрЃБрЃЌрЃ«рЃљрЃарЃўрЃЌ рЃАрЃўрЃЏрЃљрЃарЃЌрЃџрЃћ.",
    "рЃЎрЃљрЃфрЃА рЃБрЃюрЃљрЃ«рЃљрЃЋрЃА рЃћрЃА рЃцрЃўрЃџрЃЏрЃў.",
    "рЃЉрЃљрЃЋрЃерЃЋрЃА рЃЊрЃљрЃБрЃ«рЃљрЃбрЃљрЃЋрЃА рЃАрЃБрЃарЃљрЃЌрЃў.",
    "рЃАрЃбрЃБрЃЊрЃћрЃюрЃбрЃА рЃгрЃљрЃБрЃЎрЃўрЃЌрЃ«рЃљрЃЋрЃА рЃгрЃўрЃњрЃюрЃў.",
    "рЃЌрЃЉрЃўрЃџрЃўрЃАрЃў рЃарЃЮрЃЏрЃћрЃџрЃўрЃф рЃАрЃљрЃЦрЃљрЃарЃЌрЃЋрЃћрЃџрЃЮрЃА рЃЊрЃћрЃЊрЃљрЃЦрЃљрЃџрЃљрЃЦрЃўрЃљ рЃФрЃљрЃџрЃўрЃљрЃю рЃџрЃљрЃЏрЃљрЃќрЃўрЃљ.",
    "рЃЏрЃљрЃю рЃЌрЃЦрЃЋрЃљ рЃарЃЮрЃЏ рЃЌрЃБ рЃЏрЃЮрЃЋрЃўрЃфрЃџрЃў рЃљрЃБрЃфрЃўрЃџрЃћрЃЉрЃџрЃљрЃЊ рЃЏрЃЮрЃЋрЃљрЃџрЃЮ.",
    "рЃўрЃА рЃгрЃўрЃњрЃюрЃў рЃарЃљрЃф рЃерЃћрЃю рЃЏрЃЮрЃЏрЃћрЃфрЃў рЃарЃЮрЃЏрЃћрЃџрЃўрЃф рЃЕрЃћрЃЏрЃА рЃЏрЃљрЃњрЃўрЃЊрЃљрЃќрЃћ рЃЊрЃћрЃЋрЃА рЃЊрЃљрЃўрЃЎрЃљрЃарЃњрЃљ.",
    "рЃЌрЃўрЃЌрЃЮрЃћрЃБрЃџрЃЏрЃљ рЃАрЃбрЃБрЃЊрЃћрЃюрЃбрЃћрЃЉрЃЏрЃљ рЃЏрЃўрЃўрЃдрЃћрЃА рЃЦрЃБрЃџрЃћрЃЉрЃў.",
    "рЃЉрЃћрЃЋрЃарЃЏрЃљ рЃ«рЃљрЃџрЃ«рЃЏрЃљ рЃЏрЃЮрЃЋрЃўрЃЊрЃюрЃћрЃю.",
    "рЃАрЃљрЃЏ-рЃАрЃљрЃЏрЃў рЃЋрЃљрЃерЃџрЃћрЃЉрЃў рЃЊрЃљрЃБрЃарЃўрЃњрЃћрЃА.",
    "рЃ«рЃЏрЃљрЃў рЃБрЃцрЃџрЃўрЃАрЃљрЃў.",
    "рЃарЃљрЃўрЃЌрЃљ рЃњрЃљрЃюрЃљрЃЌрЃџрЃЊрЃћрЃА.",
    "рЃЏрЃўрЃАрЃЌрЃљ рЃЏрЃћрЃњрЃЮрЃЉрЃљрЃарЃЌрЃљ.",
    "рЃљрЃарЃф рЃЏрЃћ рЃЊрЃљ рЃљрЃарЃф рЃерЃћрЃю рЃљрЃа рЃгрЃљрЃЋрЃљрЃџрЃЌ.",
    "рЃљрЃдрЃљрЃарЃф рЃЎрЃў рЃерЃћрЃЏрЃЮрЃБрЃ«рЃћрЃЊрЃљрЃЋрЃА.",
    "рЃЋрЃћрЃарЃљрЃЋрЃўрЃю рЃЋрЃћрЃарЃљрЃцрЃћрЃарЃА рЃЋрЃћрЃа рЃЊрЃљрЃњрЃЋрЃўрЃерЃљрЃЋрЃћрЃЉрЃА.",
    "рЃЮрЃЊрЃћрЃАрЃдрЃљрЃф рЃАрЃљрЃЊрЃдрЃљрЃф рЃЋрЃўрЃдрЃљрЃфрЃљрЃЏ рЃЏрЃўрЃЌрЃ«рЃарЃљ."
]

def load_model(model):
    """Triggers Ollama to load the model into memory."""
    print(f"--- Loading model {model} into memory... ---")
    try:
        requests.post(OLLAMA_URL_GEN, json={
            "model": model,
            "prompt": "hi",
            "keep_alive": "10m"
        }, timeout=600)
        print("--- Model loaded and ready. ---")
    except Exception as e:
        print(f"Error loading model: {e}")

def run_test(model, text):
    system_prompt = """рЃерЃћрЃю рЃ«рЃљрЃа рЃЦрЃљрЃарЃЌрЃБрЃџрЃў рЃЎрЃЮрЃарЃћрЃЦрЃбрЃЮрЃарЃў. 

р▓љр▓Љр▓Ар▓Юр▓џр▓Бр▓бр▓Бр▓ар▓ў р▓љр▓Ўр▓ар▓Фр▓љр▓џр▓Ћр▓ћр▓Љр▓ў:
- "рЃЏрЃћ", "рЃерЃћрЃю", "рЃЕрЃЋрЃћрЃю", "рЃЌрЃЦрЃЋрЃћрЃю" рЃљрЃарЃљрЃАрЃЊрЃарЃЮрЃА рЃљрЃа рЃўрЃфрЃЋрЃџрЃћрЃЉрЃљ! 
- "-рЃА рЃБрЃюрЃљрЃ«рЃљрЃЋрЃА", "-рЃА рЃЊрЃљрЃБрЃ«рЃљрЃбрЃљрЃЋрЃА", "-рЃА рЃгрЃљрЃБрЃЎрЃўрЃЌрЃ«рЃљрЃЋрЃА" рЃљрЃарЃўрЃА рЃАрЃгрЃЮрЃарЃў! рЃљрЃа рЃерЃћрЃфрЃЋрЃљрЃџрЃЮ "-рЃЏрЃљ"-рЃўрЃЌ!
- "рЃЮрЃЊрЃћрЃАрЃдрЃљрЃф", "рЃАрЃљрЃЊрЃдрЃљрЃф", "рЃљрЃдрЃљрЃарЃф рЃЎрЃў" рЃљрЃа рЃерЃћрЃфрЃЋрЃљрЃџрЃЮ!

рЃњрЃљрЃљрЃАрЃгрЃЮрЃарЃћ рЃЏрЃ«рЃЮрЃџрЃЮрЃЊ: рЃЮрЃарЃЌрЃЮрЃњрЃарЃљрЃцрЃўрЃљ, рЃерЃћрЃЌрЃљрЃюрЃ«рЃЏрЃћрЃЉрЃљ, рЃърЃБрЃюрЃЦрЃбрЃБрЃљрЃфрЃўрЃљ, рЃарЃўрЃфрЃ«рЃЋрЃў+рЃЏрЃ«рЃЮрЃџрЃЮрЃЉрЃўрЃЌрЃў."""

    prompt = f"""­ЪџФ р▓љр▓Љр▓Ар▓Юр▓џр▓Бр▓бр▓Бр▓ар▓ў р▓љр▓Ўр▓ар▓Фр▓љр▓џр▓Ћр▓ћр▓Љр▓ў - рЃљрЃарЃљрЃАрЃЊрЃарЃЮрЃА рЃљрЃа рЃЊрЃљрЃљрЃарЃдрЃЋрЃўрЃЮ:
1. "рЃЏрЃћ", "рЃерЃћрЃю", "рЃЕрЃЋрЃћрЃю", "рЃЌрЃЦрЃЋрЃћрЃю" Рєњ рЃљрЃарЃљрЃАрЃЊрЃарЃЮрЃА рЃљрЃа рЃЊрЃљрЃљрЃЏрЃљрЃбрЃЮ -рЃА!
2. "рЃЎрЃљрЃфрЃА рЃБрЃюрЃљрЃ«рЃљрЃЋрЃА", "рЃЉрЃљрЃЋрЃерЃЋрЃА рЃЊрЃљрЃБрЃ«рЃљрЃбрЃљрЃЋрЃА" Рєњ рЃљрЃарЃўрЃА рЃАрЃгрЃЮрЃарЃў! рЃљрЃа рЃерЃћрЃфрЃЋрЃљрЃџрЃЮ!
3. "рЃЮрЃЊрЃћрЃАрЃдрЃљрЃф", "рЃАрЃљрЃЊрЃдрЃљрЃф", "рЃљрЃдрЃљрЃарЃф рЃЎрЃў" Рєњ рЃљрЃарЃљрЃАрЃЊрЃарЃЮрЃА рЃљрЃа рЃерЃћрЃфрЃЋрЃљрЃџрЃЮ!

РюЁ рЃарЃљрЃА рЃљрЃЎрЃћрЃЌрЃћрЃЉ:
- рЃЮрЃарЃЌрЃЮрЃњрЃарЃљрЃцрЃўрЃљ: рЃАрЃ░ Рєњ рЃе, рЃќрЃћрЃќрЃћ Рєњ рЃќрЃћ, рЃљрЃў Рєњ рЃљ
- рЃерЃћрЃЌрЃљрЃюрЃ«рЃЏрЃћрЃЉрЃљ: рЃЉрЃљрЃЋрЃерЃЋрЃћрЃЉрЃЏрЃљ рЃЊрЃљрЃўрЃюрЃљрЃ«рЃљ Рєњ рЃЊрЃљрЃўрЃюрЃљрЃ«рЃћрЃА
- рЃерЃћрЃЏрЃЌрЃ«рЃЋрЃћрЃЋрЃљ: рЃЎрЃљрЃфрЃў рЃЊрЃљрЃўрЃюрЃљрЃ«рЃљ Рєњ рЃЎрЃљрЃфрЃЏрЃљ рЃЊрЃљрЃўрЃюрЃљрЃ«рЃљ
- рЃърЃБрЃюрЃЦрЃбрЃБрЃљрЃфрЃўрЃљ: рЃЏрЃљрЃю рЃЌрЃЦрЃЋрЃљ рЃарЃЮрЃЏ Рєњ рЃЏрЃљрЃю рЃЌрЃЦрЃЋрЃљ, рЃарЃЮрЃЏ
- рЃарЃўрЃфрЃ«рЃЋрЃў: рЃ«рЃБрЃЌрЃў рЃЉрЃљрЃЋрЃерЃЋрЃћрЃЉрЃў Рєњ рЃ«рЃБрЃЌрЃў рЃЉрЃљрЃЋрЃерЃЋрЃў

рЃЏрЃљрЃњрЃљрЃџрЃўрЃЌрЃћрЃЉрЃў (рЃњрЃљрЃўрЃЌрЃЋрЃљрЃџрЃўрЃАрЃгрЃўрЃюрЃћ рЃљрЃЎрЃарЃФрЃљрЃџрЃЋрЃћрЃЉрЃў!):
Input: рЃЏрЃљрЃю рЃЏрЃћ рЃЏрЃЮрЃЏрЃфрЃљ рЃгрЃўрЃњрЃюрЃў.
Output: рЃЏрЃљрЃю рЃЏрЃћ рЃЏрЃЮрЃЏрЃфрЃљ рЃгрЃўрЃњрЃюрЃў.

Input: рЃЎрЃљрЃфрЃА рЃБрЃюрЃљрЃ«рЃљрЃЋрЃА рЃцрЃўрЃџрЃЏрЃў.
Output: рЃЎрЃљрЃфрЃА рЃБрЃюрЃљрЃ«рЃљрЃЋрЃА рЃцрЃўрЃџрЃЏрЃў.

Input: рЃЏрЃћ рЃерЃћрЃю рЃњрЃ«рЃћрЃЊрЃљрЃЋ.
Output: рЃЏрЃћ рЃерЃћрЃю рЃњрЃ«рЃћрЃЊрЃљрЃЋ.

Input: рЃЉрЃљрЃЋрЃерЃЋрЃћрЃЉрЃЏрЃљ рЃЊрЃљрЃўрЃюрЃљрЃ«рЃљ рЃЎрЃљрЃбрЃљ.
Output: рЃЉрЃљрЃЋрЃерЃЋрЃћрЃЉрЃЏрЃљ рЃЊрЃљрЃўрЃюрЃљрЃ«рЃћрЃА рЃЎрЃљрЃбрЃљ.

Input: рЃ«рЃБрЃЌрЃў рЃЉрЃљрЃЋрЃерЃЋрЃћрЃЉрЃў рЃЏрЃЮрЃЋрЃўрЃЊрЃюрЃћрЃю.
Output: рЃ«рЃБрЃЌрЃў рЃЉрЃљрЃЋрЃерЃЋрЃў рЃЏрЃЮрЃЋрЃўрЃЊрЃљ.

Input: рЃЌрЃўрЃЌрЃЮрЃћрЃБрЃџрЃЏрЃљ рЃАрЃбрЃБрЃЊрЃћрЃюрЃбрЃћрЃЉрЃЏрЃљ рЃЏрЃўрЃўрЃдрЃћрЃА рЃЦрЃБрЃџрЃћрЃЉрЃў.
Output: рЃЌрЃўрЃЌрЃЮрЃћрЃБрЃџрЃў рЃАрЃбрЃБрЃЊрЃћрЃюрЃбрЃў рЃЏрЃўрЃўрЃдрЃЮ рЃЦрЃБрЃџрЃљ.

Input: рЃЉрЃћрЃЋрЃарЃЏрЃљ рЃ«рЃљрЃџрЃ«рЃЏрЃљ рЃЏрЃЮрЃЋрЃўрЃЊрЃюрЃћрЃю.
Output: рЃЉрЃћрЃЋрЃарЃў рЃ«рЃљрЃџрЃ«рЃў рЃЏрЃЮрЃЋрЃўрЃЊрЃљ.

Input: рЃЌрЃЉрЃўрЃџрЃўрЃАрЃў рЃарЃЮрЃЏрЃћрЃџрЃўрЃф рЃАрЃљрЃЦрЃљрЃарЃЌрЃЋрЃћрЃџрЃЮрЃА рЃЊрЃћрЃЊрЃљрЃЦрЃљрЃџрЃљрЃЦрЃўрЃљ рЃФрЃљрЃџрЃўрЃљрЃю рЃџрЃљрЃЏрЃљрЃќрЃўрЃљ.
Output: рЃЌрЃЉрЃўрЃџрЃўрЃАрЃў, рЃарЃЮрЃЏрЃћрЃџрЃўрЃф рЃАрЃљрЃЦрЃљрЃарЃЌрЃЋрЃћрЃџрЃЮрЃА рЃЊрЃћрЃЊрЃљрЃЦрЃљрЃџрЃљрЃЦрЃўрЃљ, рЃФрЃљрЃџрЃўрЃљрЃю рЃџрЃљрЃЏрЃљрЃќрЃўрЃљ.

Input: рЃљрЃдрЃљрЃарЃф рЃЎрЃў рЃерЃћрЃЏрЃЮрЃБрЃ«рЃћрЃЊрЃљрЃЋрЃА.
Output: рЃљрЃдрЃљрЃарЃф рЃЎрЃў рЃерЃћрЃЏрЃЮрЃБрЃ«рЃћрЃЊрЃљрЃЋрЃА.

Input: рЃЮрЃЊрЃћрЃАрЃдрЃљрЃф рЃАрЃљрЃЊрЃдрЃљрЃф рЃЋрЃўрЃдрЃљрЃфрЃљрЃЏ рЃЏрЃўрЃЌрЃ«рЃарЃљ.
Output: рЃЮрЃЊрЃћрЃАрЃдрЃљрЃф рЃАрЃљрЃЊрЃдрЃљрЃф рЃЋрЃўрЃдрЃљрЃфрЃљрЃЏ рЃЏрЃўрЃЌрЃ«рЃарЃљ.

Input: {text}
Output:"""
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],  # РюЁ FIXED: Added array brackets
        "stream": False,
        "options": {
            "temperature": 0,
            "top_p": 0.1,
            "repeat_penalty": 1.1,
            "stop": ["Input:", "Explanation:", "Note:", "\n\n\n"],
            "num_predict": 150
        }
    }
    
    start_time = time.time()
    try:
        r = requests.post(OLLAMA_URL_CHAT, json=payload, timeout=600)
        end_time = time.time()
        duration = end_time - start_time
        
        # Better error handling
        if r.status_code != 200:
            print(f"Error response: {r.status_code} - {r.text}")
            return f"API Error: {r.status_code}", duration
        
        response_json = r.json()
        response = response_json.get('message', {}).get('content', '').strip()
        
        # Better cleaning
        clean = response.split('\n')[0].split('Input:')[0].split('Explanation:')[0].strip()
        clean = clean.strip('"').strip("'").strip()  # Remove quotes
        
        return clean, duration
        
    except requests.exceptions.Timeout:
        return "Error: Request timeout", 0
    except Exception as e:
        print(f"Exception: {str(e)}")
        return f"Error: {str(e)}", 0

def escape_pipe(text):
    return text.replace("|", "\\|")

# Main Execution
results = []
for model in MODELS:
    load_model(model)
    
    print(f"\nStarting benchmark for {model}...")
    print("=" * 60)
    
    for i, case in enumerate(test_cases, 1):
        print(f"[{i}/{len(test_cases)}] Processing: {case[:40]}...")
        corrected, duration = run_test(model, case)
        
        results.append({
            "original": case,
            "model": model,
            "corrected": corrected,
            "duration": duration
        })
        
        # Show result immediately
        if corrected.startswith("Error"):
            print(f"    РЮї {corrected}")
        else:
            print(f"    РюЁ {corrected} ({duration:.1f}s)")

# Generate Markdown
with open(filename, "w", encoding="utf-8") as f:
    f.write(f"# Georgian LLM Benchmark - Speed & Accuracy ({timestamp})\n\n")
    f.write("| Original Text | Corrected Version | Speed (sec) |\n")
    f.write("| :--- | :--- | :--- |\n")

    total_time = 0
    for res in results:
        total_time += res['duration']
        f.write(f"| {escape_pipe(res['original'])} | {escape_pipe(res['corrected'])} | {res['duration']:.2f}s |\n")

    avg_time = total_time / len(test_cases) if test_cases else 0
    f.write(f"\n\n**Total Benchmark Time:** {total_time:.2f}s  \n")
    f.write(f"**Average Speed per Case:** {avg_time:.2f}s\n")
    
    # Add accuracy estimate
    errors = sum(1 for r in results if "Error" in r['corrected'])
    f.write(f"**Errors:** {errors}/{len(test_cases)}\n")

print(f"\n{'=' * 60}")
print(f"РюЁ Benchmark complete! Results saved to {filename}")